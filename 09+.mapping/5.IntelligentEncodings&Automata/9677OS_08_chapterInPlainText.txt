8
Intelligent Encodings and Automata
This chapter aspires to both introduce and familiarize the reader with more advanced concepts, such as statistical data analyses, textual parsing and with ways to implement intelligent encodings. We will, further, examine the concept of automaton and demonstrate how we can implement autonomous systems that generate audiovisual structures on their own. Yet, it has to be stretched that this chapter serves primarily as a pragmatic introduction rather than as a formal treatise to the aforementioned. Even if I did my best to ensure that the examples heretofore are indicative of both the complexities as well as the potential of the topics discussed, those interested in an in-depth discussion of the technical challenges involved in any of those areas should consult more specialized resources.
The topics that will be covered are:
•	Statistical analysis and probability distributions.
•	Textual parsing.
•	Intelligent encodings.
•	Neural networks.
•	Cellular automata.
•	The game of life.

Analyzing data
In the 6th chapter, we discussed how to acquire data as well as how to generate them by means of machine listening techniques. It is also often the case that we need to analyze non-audio signals or data collections of some sort. However, data-analysis stands for an infinite range of operations we may perform on some collection, and more to this, it is often the case that we blindly probe the collection for potentially interesting patterns rather than looking for something in particular. Dealing with such cases in real-life projects would be overwhelming if there were no generalized methodologies to serve as a starting point.
Fortunately there is already a kind of science dedicated to the systemic study of the collection, organization, analysis, interpretation and presentation of any kind of data, namely statistics. As such, it provides us a very sophisticated background to perform analyses and feature extraction of various sorts. By applying statistical analysis on our datasets we can easily interpret our data with respect to some desired feature (as long as we can mathematically formalize the latter) as well as probe it for interesting behavior by means of calculating some standard measures. We will discuss the most fundamental concepts and techniques heretofore, which we can combine to achieve even more complicated analyses.
Statistical analyses and meta-data
Let us introduce ourselves with some fundamental statistical notions and measures: The mode of a data collection is the value with highest probability, or in other words, the value that appears more often. The opposite of the mode is the usually referred to as the least repeated element. Note that, paradoxically, in a list of numbers wherein none is repeated and wherein all values have equal chances of appearing, the mode is also the least repeated number. The head of a dataset stands for those values that appear quite often, and the tail for the remaining. Mean is the average of the data collection in question. Median is that value separating the higher from the lower half of the dataset, or in other words the ‘middle-value’ of the dataset. Range is simply the distance of the lowest to the highest number. Since the later will be very misleading if our dataset includes just a couple of very big or very small numbers, therefore interquartile range (usually abbreviated as iqr), defined as the distance between the upper and the lower quartile has been also introduced. Variance stands for the average of the squared differences from the mean. Standard deviation or σ (the Greek letter sigma) is the square root of the variance, and hence another measure of dispersion. Of course most of these measures are meaningful only for numerical data. In the following code we demonstrate how to calculate them for our arrhythmia dataset. Note that apart from the standard select and reject instance methods, we also use maxItem and minItem, which will return the item that will give the maximum or minimum, respectively, result when passed to the supplied Function.
( // calculate statistical meta-data
var data, mode, leastProbableNumber, head, tail, mean, median, range, iqr, variance, deviation;

// first load and prepare our dataset
data = CSVFileReader.read("~/arrhythmia.data".absolutePath,true,true); // read from file
data = data[0].collect(_.asInteger); // consider just a chunk and convert its elements to Integers
data = data.select(_!=0); // remove zeros

// calculate meta-data
mode = data.maxItem({arg item; data.occurrencesOf(item)}); 
("Mode is: " + mode).postln; 
leastProbableNumber = data.minItem({arg item; data.occurrencesOf(item)});
("Least Probable number is: " + leastProbableNumber).postln; 
head = data.select{arg item; data.occurrencesOf(item) >= 6}; // only those values that appear at least 6 times
("Head is: " + head.as(IdentitySet)).postln; 
tail = data.reject{arg item; data.occurrencesOf(item) >= 6}; // values that appear less than 6 times
("Tail is: " + tail.as(IdentitySet)).postln;
mean = data.sum / data.size; // the sum of all data divided by the size of the dataset
("Mean is: " + mean).postln;
median = data.sort[data.size/2]; // the 'middle' element when the array is sorted 
("Median is: " + median).postln;
range = data.max - data.min; // range
("Range is: " + range).postln;
iqr = data.at(( (data.size/4) .. ((data.size*3)/4) )); // return an array with only the second and the third quartilion
iqr = iqr.max - iqr.min; // calculate iqr range
("Interquartile Range is: " + iqr).postln;
variance = (data.collect{arg item; (item-mean).squared}).sum/data.size; // calculate variance
("Variance is: " + variance).postln;
deviation = variance.sqrt; // calculate deviation
("Deviation is: " + deviation).postln;
)
Calculating those measures essentially results in generating meta-data (to be precise, descriptive meta-data), which is a fundamental concept for statistics and data-analysis in general. 
Meta-data stand for data that represent or abstract characteristics, properties or attributes of other data and which usually originate from the analysis of other.data. 
Probabilities and histograms
Two very important statistical notions are those of probability and of probability distribution. Probability is a measure of how likely it is for an event to happen. When dealing with discrete datasets, an event would be to retrieve the next element of a dataset. We can easily calculate probabilities by simply dividing the occurrences of a specific element within our dataset and dividing it with the latter’s total size. Graphs of elements (horizontal dimension) versus their occurrences within a dataset are termed histograms and are extremely useful in allowing one to overview the probability distribution of all elements in a dataset. Naturally, non-existent elements are represented by a probability of 0. In the following example we calculate the probability distribution as an array of as many indices as the range of possible values in the original dataset and with entries representing how many instances of each particular index are contained in the latter. Of course when we are dealing with negative values we need to bias everything accordingly and then compensate for it on the Plotter using domainSpecs instance variable to set a new horizontal range. As of writing, however, this approach will fail to properly set up the values due to an internal bug which is to be fixed in some future version of SuperCollider. 
( // calculate a histogram
var data, histogram, histSize;
data = "curl \"http://www.random.org/integers/?num=1000&min=-100&max=100&col=1&base=10&format=plain&rnd=new\" ".unixCmdGetStdOutLines; // retrieve random numbers in the range (-100,100) from random.org
data = data.collect(_.asInteger); // convert to integers
histSize = data.max-data.min + 1; // calculate the size 
histogram = Array.fill(histSize,{0}); // a signal with as many elements as the range of values we are interested in
data.do({ arg item;  
	var count, histoIndex; 
	histoIndex = item + data.min.abs; // to compensate for negative items
	count = histogram.at(histoIndex); // read previous value
	histogram.put(histoIndex, count + 1); // increment it
});
histogram.plot().domainSpecs_([-100,100,\lin,1].asSpec); // make a histogram
)
In probability theory, independent events are those that are not affected, in any possible way, by other events, for instance any toss of a coin has exactly 50% probability of being heads or tails. We can also speak of joint probability, i.e. the probability of more than one event: e.g. what is the probability of the next three items retrieved from a set to be of particular values. We can calculate joint probabilities simply by multiplying together the probabilities of each individual event. Or in mathematical notation: P(A and B)  = P(A) x P(B); wherein P(A) stands for the probability of event A.  Likewise, dependent events are events dependent upon other events. For example, when we are iterating through a dataset rather than randomly asking numbers out of it, the probability of item having a certain value depends not solely upon the number of its occurrences within this dataset, but also upon how many times this value has been already retrieved and how many items are left in the dataset; in principle we would have to recalculate its probability for the remaining dataset before we can tell the actual probability. In the case of dependent events, we can speak of conditional probability, which stands for the probability of an event given some other condition. In mathematical terms we can calculate the probability of A given B like this: P(A | B) = P(A and B) / P(A). By means of these simple rules we can calculate the probability of complicated events and implement algorithms that target very specific cases. However, always bear in mind that probabilities are just indicators, and some time erroneous ones: it could be that the next element in a dataset is one with a probability of only 0.1%.
Dealing with textual datasets
Hitherto we have only dealt with numerical datasets. It is, nonetheless, quite common to deal with a certain kind of non-numerical data, namely with text. Depending on the specifics of each application and what kind of information we are interested in extracting, dealing with textual datasets could be anything from extremely simple to overwhelmingly complicated. For instance, we can easily calculate how probable it is for a certain string to appear by means of counting their occurrences in a dataset and then we can even calculate their probability distributions or map them to audio synthesis parameters. Yet, it would be extremely challenging, if not completely impossible with the current technology, to automatically synthesize the abstract of this chapter given its contents as a String. Typically, performing sophisticated tasks with text involves stages such as lexical analysis, syntactical analysis (or parsing) and semantical analysis, which are complicated enough to be the subject of dedicated books and, hence, impossible to discuss in depth herein. 
Let us consider a quite complicated, albeit very useful, syntactical analysis problem: Assuming that the whole text of this chapter is stored in some plain text file, how can we analyze it so that we only extract only those blocks of text that are valid SuperCollider code and later evaluate them at will ? Given that SuperCollider is able to both parse and lexically analyze text formatted as valid SuperCollider code, all we need to do is analyze the text, identify such blocks and  encapsulate them as individual String objects;  then we can simply invoke their interpret instance method when we need to evaluate them. To actually implement such an algorithm, we need to describe in a computer-understandable way what makes a code block different from irrelevant text. For our implementation herein we will scan the text until we identify a parentheses followed by a blank character and a comment line delimiter [that is a '( //' ], then all we need to do is find the match of this parenthesis, which signifies the end of the block. This is not necessarily a prerequisite of all valid code in SuperCollider, however thoughout this chapter we have followed the convention that all standalone examples are formatted this way. We can implement a basic parentheses matching algorithm if we simply increment a counter for every opening parentheses and decrement it for every closing one; when the counter equals 0 we know that we have found the ending of the code block in question. There is still a problem, though: since we have already used the '( //' token in this chapter and outside the context of a code block (like in this very sentence) and since within the code examples parentheses could be (are actually in this case) contained within Strings, comments, Symbols or Chars that are neither necessarily matched neither signify the opening of a block of code.
For the examples of this chapter we have intentionally enclosed those off-code-block appearances in quotes and make sure that the in-code ones are either matched or preceded by a quote or a $ symbol, so that with the addition of some simple rules we can safely ignore them.
The code follows:
( // extract and evaluate code from text file
var file, path, text; // used to read the text from file
var cues; // of initial position of and '( //'
var chunks; // array with chunks of text containing potential code
var code;  // an array with the parsed code

path = Document.current.dir +/+ "9677OS_08_chapterInPlainText.txt"; // the path to the file
file = File(path,"r"); // open for reading operations 
text = file.readAllString; // read all text to a string
file.close; // close files

cues = text.findAll("( //"); // find the positions of all occurences of '( //'
cues = cues.select{ arg index; (text[index - 1] != $') && (text[index - 1] != $") }; // remove all invalid parenthesis (ones preceded by ' or ")

(cues.size-1).do{ arg index; 
	chunks = chunks.add(text[(cues[index] .. cues[index+1])].toString); // copy all text between subsequent cues and put it on chunks array
}; 
chunks = chunks.add(text[(cues.last .. text.size)].toString);  // also add last chunk

chunks.do{ arg item, index; // for every chunk of text
	var counter = 0, position = 0, done = false;
	item.do{arg char,i; // for every character in chunk
		if (done.not) { // if not done, increment counter for every '(' and decrement it for every ')'
			case 
			{char == $( } { counter = counter + 1 }
			{char == $) } { counter = counter - 1 }; 
			if (counter == 0) {position = i; done = true;}; // if counter equals 0, then the code ends at position i and the done flag is set to true 
		}
	};
	code = code.add(item[(0 .. position)].toString); // copy the parsed code to the code array
};

(code.size + " blocks of code have been successfully extracted from text file").postln;
"Fourth code block will be now evaluated".postln;
code[0].interpret; // evaluate first example
)
However simplistic this example is, having to correctly parse and evaluate code sent to supercollider from some remote client is a real-life scenario, or at least something that I personally have had to do many times in various projects. It does not necessarily make sense to do something like that, yet it is theoretically possible to even implement our own programming language within SuperCollider so that the latter would correctly parse and translate it to equivalent code that could later be evaluated by the SCLang. 
Advanced Mappings
In Chapter 6: Data Acquisition and Mapping, we demonstrated how we can essentially map any consecutive range to any other with respect to distribution curves. In this section we will extend our arsenal of encoding techniques and introduce ourselves with how to implement complex and intelligent encodings.
Complex and intelligent encodings
There are situations wherein what we need is some kind of intelligence that will take the necessary decisions and select the appropriate process from a broader range of candidate ones in order to encode our data properly. To realize such mappings we need some kind of mechanism that ensures the right decisions are taken, and of course we need to define alternative behaviors. A simplistic way to implement decision-making algorithms would be using test mechanisms and control flow structures, such as if or case. For the following simplistic example, assume, that we want to sonify floating point numerical values in the range of 0-1 so that they control oscillators that are either in a low (200-400) or in a high (2000-4000) frequency register. That is to say that our destination range is not continuous. 
Consider this possible solution:
( // simple decision-making encoder
Server.default.waitForBoot({
	var data = Array.fill(100,{rrand(0,1.0)}); // our dataset
	var mappingFunc = { arg datum; // the mapping function 
		if (datum<=0.5) { // if input is less than 0.5
			datum.linlin(0,0.5,200,400); // map linearly in the low register
		} {  // else map linearly to the high register
			datum.linlin(0.5,1.0,2000,4000);
		};
	};
	fork{loop{ // sonify dataset
		var freq;
		freq = mappingFunc.(data.choose);
		{SinOsc.ar(freq) * Line.ar(1,0,0.4,doneAction:2)}.play;
		0.5.wait;
	}};
})
)
This is a very simplistic case of course, albeit it exemplifies how an algorithm can make decisions and is therefore a very primitive kind of artificial.intelligence. In the next example our algorithm is intelligent enough to dynamically take more sophisticated decisions and with respect to analyzing the probability distribution of the remaining values in the dataset each time a value is used. The algorithm encodes data according to how probable it is to trigger a certain frequencies in the output. Let us examine the various parts of the algorithm. The input data is just integer numerical values between 1 and 14 that we may use only once each. Then there are three possible ways to map the input data: frequencies that correspond to notes from the C major scale, with the most probable notes to be C,E,G and B, frequencies that correspond to the Eb minor scale with the most prominent to be Eb,Gb,Bb,Db, or just a random frequency. The 7 most prominent values (the statistical head) will be mapped to behavior A, then if the probability of the 3 next consecutive values to be within these 7 values is greater than 60%, the rest values are all mapped to a random number, else they are mapped to the Eb minor group. To implement such an algorithm and since the input range may vary, we need a function that will dynamically ‘plug’ the input to the output range and perform the right encoding, as well as another function to calculate the probabilities and dynamically call the former with the right arguments.
In the core of our implementation we have the following mapping Function:
// mapping function
var mappingFunc = { arg data;
	var head, tail, prob, choice, freq, index;
	#head, tail = headTailFunc.(data); // calculate head and tails
	prob = probFunc.(data,head); // calculate the probality of the next 3 consecutive values to be in the head
	index = data.size.rand; // a random index in the dataset
	choice = data[index]; // pick a random value at index
	freq = if (head.includes(choice)) { // if chosen datum is in the head
		majorFunc.(head,choice); // call majorFunc
	} { 
		if (prob > 0.29) { // else if the probability of the next 3 consecutive to be in the head is more than 29%
			randFunc.(); // produce a random value
		} { // else 
			minorFunc.(tail,choice); // call minorFunc
		}
	};
	data.removeAt(index); // remove datum from dataset
	freq; // return frequency
};
Which relies on a series of auxiliary Functions, such as headTailFunc (calculates the statistical head and tail), probFunc (calculates the probability of the next 3 consecutive values to be in the head) and a series of encoders, namely majorFunc, randFunc and minorFunc, which will return the actual frequency to play. Since mappingFunc returns the frequency value, we can then simply play a sound like this:
// sound
fork{100.do{ var freq;
	freq = mappingFunc.(dataset); // encode data
	{SinOsc.ar(freq) * Line.ar(1,0,0.4,doneAction:2)}.play; // play sound
	[0.5,0.25,1].wchoose([0.6,0.3,0.1]).wait; // wait 
}}; 
As far as the encoders are concerned, their implementation is rather trivial. randFunc simply returns a random frequency while majorFunc and minorFunc first convert choice into a degree of the scale in question and then convert the latter into a value representing frequency. 
The code for the auxilary Functions is given below:
// function calculate the head and Tail of the dataset
var headTailFunc = { 
	arg data; // calculate head and tail of the dataset
	var head, tail, sorted;
	sorted = data.asBag.contents.asSortedArray.sort({arg a,b; b[1]<a[1]}).collect(_[0]); // sort dataset so that the most probable values are first
	head = sorted.clipAt((0 .. 6)); // the 7 most probable values are the head 
	tail = sorted.clipAt((7 .. 13)); // the rest are the tail
	[head,tail]; // return an array with the head and tail
};

 // function to calculate the probability of the next 3 consecutive values to be in the head
var probFunc = { arg data, head; 
	var prob;
	prob = head.sum(data.occurrencesOf(_)/data.size); // probability of next value to be in head
	prob.cubed; // return the probability of 3 consecutives values to be in the head
};
The most complicated part is probably the highlighted line above, wherein we convert the dataset into a SortedArray containing all the possible different values the original dataset consisted of sorted by probability. asBag.contents will return an instance of Dictionary wherein possible entry in the original dataset points at its very number of occurrences. We then convert these keys/values pairs to duplets within a SortedArray to which we apply our custom sorting algorithm so that the former are sorted with respect to how often an element occurs in the original dataset. The last part is to collect only these elements (and not their number of occurrences). The complete code for this example can be found online in the packt code bundle for this book.
Neural Networks
In computer science artificial neural networks (aka ANN) are a fundamental machine learning technique used whenever we want to achieve predictive modeling, adaptive control of some structure and, in short, in all those cases when we want to grant our algorithm the ability to learn on its own according to data-input. Largely inspired by their biological equivalents, such networks operate via the flow of signals through individual neurons interconnected to greater structures rather than according to traditional computation paradigms. ANNs are extremely powerful and currently the only feasible way to solve certain kinds of problems. Typical examples are complicated pattern recognition or data classification problems. Consider for example a software application that identifies hand-written text and converts it to digital files. It is impossible to teach computers how to perform such a complex task since we do not even properly understand how we do it ourselves. It is therefore impossible to describe an algorithm by traditional imperative or functional means. An ANN approach would be to provide the software with a large enough sample of successfully identified text and let it figure out itself how to get there, or in other words, let it learn.
There are several types of ANNs which may have either supervised, unsupervised or reinforced learning abilities. We will limit our discussion here to the arguably simplest of all families of ANNs, namely feedforward neural networks. Such a network typically consists of an input layer, an output layer and an arbitrary number of layers in-between, which are broadly referred to as hidden layers. These layers consist of an arbitrary number of nodes, each of which is interconnected to other according to the way each individual ANN is designed. Yet, in feedforward neural networks, as the name suggests and unlike biological neural networks, dataflow always occurs towards one direction; there is no recursion. Such networks have the ability to learn; actually we have to train them before we use them. At any given time the nodes in the hidden layers process input by means of some activation function (there are several alternative ones) and with respect to a set of associated weights values. Initially, when the ANN is still ‘young’, these weights correspond to random values, during a single training cycle, aka an epoch, we feed the network with input data, let them flow towards the output manipulated accordingly by the hidden layers and then we compare the output with the desired one. The error is calculated and the information is back-propagated accordingly to update all the weights, so that when the input is fed back to the network slightly better results will be achieved. Training a neural network typically involves several thousands epochs. Depending the kind of problem, even millions or more of epochs may be required making the training a significantly time-consuming process for complex problems. Nevertheless it does make sense to get into all the trouble of training an ANN, since when done we are left with a specialized brain capable of solving a very certain kind of problem, even if cannot formalize a working algorithm ourselves.
Fortunately, a single-layered feedforward neural network is being already implemented for SuperCollider by Nick Collins and can be found in his SCMIR library which is available for download from: http://www.sussex.ac.uk/Users/nc81/code.html#SC Albeit rudimentary, NeuralNet is a fully functional ANN which we can use to add intelligence to our projects. First thing to do is let the class know the path of the NeuralNet unix executable (bundled with SCMIR library) like this:
NeuralNet.pathToNeuralNetBinary_("/Users/marinos/Library/Application Support/SuperCollider/Extensions/SCMIRExtensions/scmirexec/NeuralNet") // set the path of the NeuralNet unix executable
This is not strictly necessary, but this way we can use the trainExt method instead of the standard train one, to train our network in significantly faster times. We only have to do this once, the class will store information internally and even if we restart SuperCollider NeuralNet will know where to find the appropriate executable. We can then create a new ANN simply proving the number of nodes for the input, hidden and output layers, as well as the desired learning rate (typically in the range of 0.01-1.0) and a factor to initialize weights (they will be initialized to random values within a ±factor range).
To train the network we need a sample of course, which should be an instance of Array containing instances of Array with the input and the output values. Obviously those arrays should contain as many elements as the corresponding number of nodes. We can then use train or trainExt to feed our data to our ANN, also providing a desired error and a maximum number of epochs. The network will keep iterating until the error in the calculations is less than the desired one or until the maximum number of epochs is achieved, posting its state at every stage so that we now what the final error is. 
A simple example:
( // a simple Neural Network Example
var net, sample; 
net = NeuralNet(2,20,1,0.01,1.0); // 2 ins, 1 out, 20hidden
sample = [ [[1,0],[1]], [[0.5,0.5],[0.5]], [[1,0.5],[0.5]], [[0,1],[0]] ]; // the sample
net.trainExt(sample,0.01,10000); // train over 10000 epochs or until the error is less that 0.01
// test
net.calculate([1,0]).postln;
net.calculate([0.5,0.5]).postln;
net.calculate([0,1]).postln;
net.calculate([0.75,0.25]).postln;
)
In my computer the final iteration resulted in Epoch: 19999,  Error: 0.0221883 so I know that this ANN is supposed to give acceptable results. And indeed, it maps, for instance [ 1, 0 ]  to 0.92 and [ 0.5, 0.5 ] to 0.43, which may not be the ideal 1 and 0.5 but are nevertheless very close and thus acceptable. At this stage our ANN has already made extrapolations and identified some underlying pattern in our sample and will map all input values accordingly. We can see for instance that it mapped values [0.25, 0.75] to 0.18, which does make sense since [0.5, 0.5] is supposed to result in 0.5 and [0,1] is supposed to result in 0 and since 0.18 is indeed somewhere in-between. This is the most intriguing aspects of ANNs, they tend to ‘understand’ data in their own way without us having to explicitly explain (or even understanding) the underlying patterns.  
Arguably, for more complex problems, which could involve millions of epochs, we will most likely want to somehow save the neural network so we can use it again without having to train it from scratch. Luckily, NeuralNet provides us with invaluable methods to access the whole ANN’s data specifications, including the weights, and to create new networks out of such specifications, namely getNN and newExisting respectively. Note also that NeuralNet will only operate with values in the 0 to 1 range; therefore we need to scale them accordingly to use it for other sorts of data. 
Machine learning is that branch of artificial intelligence dealing with the construction and study of systems that can learn from data.
A biological neuron (or neurone) is an electrically excitable cell that processes or transmits information in a human or other animal brain. In computer science, artificial neurons are mathematical functions conceived as a approximate models of biological neurons.
Automata
Automata is the plural of automaton which in Greek stands for any kind of non-biological self-operating being. When a program, or elements of it, operate on their own, either following finite behavior instructions by means of stochastic or probabilistic algorithms or relying on some sort of artificial intelligence, we can speak of an automaton. Automata in their various implementations are fundamental parts of any generative art project, that is, art created partly or exclusively relying on some sort of autonomous, non-human controlled system. As far as mapping and visualization is concerned, whenever decisions and behaviors are performed intrinsically by our program, we can speak of a generative process that, by definition, involves some kind of automaton. Herein we will pinpoint our discussion on the infamous cellular automata, while on the next chapter we will discuss other kinds of automata as well.
Cellular Automata
A cellular automaton comprises of a n-dimensional grid of cells each of which has a certain neighborhood and may alternate between a finite number of states with respect to some set of rules, usually considering the state of neighboring cells. On each generation, a cellular automaton will permute according to how each individual cell changes state, therefore generating new patterns and structures dynamically. The initial pattern (which is also referred to as the seed) is of great importance and typically decisive of how the automaton will evolve over time. In the following example we implement an elementary one-dimensional Wolfram’s cellular automaton. Herein our grid is merely a line of cells of 1-pixel width, each of which has a neighborhood of three (that is itself, the pixel on its right and that on its left) and may alternate between two possible states represented by two possible colors. Each subsequent generation will be placed beneath the first one, which is placed on top, and will have its cells configured with respect to the previous one.
In detail, the state of each cell will be a function of the individual states of the cell that constituted the former’s neighborhood in the previous generation. A rule in this context is a configuration of the possible outcomes of each possible combination. The possible states of a neighborhood of 3 wherein each cell alternates between 2 possible states, in binary notation would be:  000, 001, 010, 011, 100, 101, 111, wherein 0 represents one of the two colors and 1 the other. We can then describe a rule as another binary number which holds the results for every of these configurations. For example the rule 01011010 would mean that a cell having a neighborhood of 000, in the next generation will have a state of 0, a cell having a generation of 001 will have a state of 1, and so on. The following diagram describes the rule graphically (here black represents 0 and white 1).

 
9677OS_08_01.png
By the way, the number 01011010 corresponds to the decimal number 90 (which is arguably easier to remember). In SuperCollider we can invoke asBinaryDigits on some decimal number to get an instance of Array containing the individual bits of its binary equivalent. Therefore it does make sense to use decimals to describe rules. Configurations of 8 bits may represent a maximum of 256 (including 0) different numbers. However, out of these possible rules only a few will produce interesting results. Consider the following example wherein we use a 512 sized grid and we both visualize/sonify the results of a cellular automaton. Sonification is done in the spectral domain, using pvcalc to generate a spectrum with energy in those bins that correspond to a colored cell. Note that an instance of Control is used to allow instances of Array as arguments to our Synth; the initial value of the each parameter should be an instance of Array similar in size as that later passed as arguments; in our case we will use instances of Array comprised of two 256-sized arrays, one for the magnitudes and one for the phases. Visualization is implementing using Pen to add one-pixel sized rectangles when a cell’s value is 1, else the background color is revealed. 
The full code is given herein:
( // 1-dimension cellular automata
Server.default.waitForBoot({
	var synth; // a synth used later
	var ruleSet = [60,90,102,150].choose.asBinaryDigits; // randomly choose a rule and convert to an Array of binary digits
	var cells = Array.fill(512,{[0,1].choose([0.95,0.05])}); // a random seed of mainly 0s and just a few 1s at random places 
	var generateAccordingToRule = { arg a,b,c; // simply map each neighborhood state to each digit of our rule, respectively  
		case 
		{(a == 0) && (b == 0) && (c == 0)} {ruleSet[0]}
		{(a == 0) && (b == 0) && (c == 1)} {ruleSet[1]}
		{(a == 0) && (b == 1) && (c == 0)} {ruleSet[2]}
		{(a == 0) && (b == 1) && (c == 1)} {ruleSet[3]}
		{(a == 1) && (b == 0) && (c == 0)} {ruleSet[4]}
		{(a == 1) && (b == 0) && (c == 1)} {ruleSet[5]}
		{(a == 1) && (b == 1) && (c == 0)} {ruleSet[6]}
		{(a == 1) && (b == 1) && (c == 1)} {ruleSet[7]};
	};
	var window = Window("1-dimension cellular automata", 512@200).front.onClose_({synth.free;}); // our parent window
	var userView = UserView(window, 512@200).background_(Color.magenta).animate_(true).clearOnRefresh_(false).frameRate_(40).drawFunc_({ // setup UserView and callback func
		var counter = userView.frame % 200; 
		synth.set(\array, cells); // modulate synth
		512.do{ arg i; 
			// first draw each cell
			if (cells[i].asBoolean) {
				Pen.fillColor_(Color.yellow);
				Pen.addRect(Rect(i,counter,1,1));
				Pen.fill;
			};
			// then calculate next generation
			cells[i] = generateAccordingToRule.value(cells.foldAt(i-1),cells[i],cells.foldAt(i+1));
		};
		// when we have reaches the botom start from scratch with a new random rule and random seed
		if (counter == 0) {
			userView.clearDrawing; // clear previous contents
			ruleSet = [60,90,102,150].choose.asBinaryDigits; // randomly choose a rule and convert to an Array of binary digits

			cells = Array.fill(512,{0}); // an array of empty cells
			rrand(1,50).do{ // add a random number of 1s at random places to achieve a random seed
				cells[512.rand] = 1; 
			};
		};
	});	
	fork {  // sound 
		SynthDef(\caSynth, { // synthDef
			var signal, array, magnitudes, phases;
			array = Control.names([\array]).kr(Array.fill(512,{0})).clump(2).flop; // Control is used to allow an array to be passed as an argument
			magnitudes = array[0]; // read argument magnitudes
			phases = array[1]; // read argument phases
			signal = Silent.ar(); // a silent signal since we will replace it
			signal = FFT(LocalBuf(512),signal); // FFT
			signal = signal.pvcalc(512,{
				[magnitudes,phases]; // manually set magnitudes and phases
			});
			signal = IFFT(signal); // inverse FFT
			Out.ar(0,signal!2);
		}).add; // add SynthDef
		Server.default.sync; // sync with Server
		synth = Synth(\caSynth); // start synth
	};
});
)
A still from this visualization is shown below: 
 
9677OS_08_02.png
Cellular automata are known since the 40s, primarily due to Stanisław Ulam and John von Neumann who first discovered and systematically studied them at the Los Alamos National Laboratory in New Mexico. It wasn’t but until the 70s, however, that cellular automata were popularized and exceeded narrow academic circles, primarily due to the infamous Conway’s game of life, which we will soon discuss in more detail. A seminal work in the subject is Stephen Wolfram’s 1,280-page book entitled A New Kind of Science. Wolfram argues that cellular automata are relevant to the study of biology, chemistry, physics and several other branches of science. The entire book is freely available online at http://www.wolframscience.com/nksonline/toc.html
Game of life
The infamous game of life is a two-dimensional cellular automaton originally devised by the British mathematician John Horton Conway. Herein we have a two-dimensional grid, two-state cells (alive or dead in this context) and a 9-cell neighborhood (the cell in question and all its neighboring ones up, down, left, right and diagonally). There are four rules:  
1.	Loneliness: any living cell with fewer than two living neighbors dies.
2.	Stasis: any living cell with two or three living neighbors lives on to the next generation.
3.	Over-population: any living cell with more than three live neighbors dies.
4.	Birth: any dead cell with exactly three living neighbors becomes a living cell.
What is particularly intriguing with the game of life is that there are certain patterns which will constantly oscillate between the same states, certain others which will remain static, certain others which appear as if moving. To implement a basic game of life we will follow a similar approach as before, this time however using a two-dimensional array of cells:
cells=Array.fill(32,{Array.fill(16,{[1,0].wchoose([0.3,0.7])})});
and an updateCell function instead of a ruleset, like this:
updateCell = {arg xIndex,yIndex; // function to count neighbours and update cells' state
	var neighbours = 0 ; // initial number of neighbours
	var state = cells[xIndex][yIndex]; // set current state
	var newState = state; // new state
	// first count neighbours
	[-1,0,1].do{arg i; 
		[-1,0,1].do{arg j; 
			neighbours = neighbours +
			cells.foldAt(xIndex+i).foldAt(yIndex+j);
		}
	};
	
	if (state.asBoolean) {neighbours = neighbours - 1}; // if state is not 0, subtract cell's own state 
	case  // calculate new state
	{state.asBoolean && (neighbours < 2)} {newState = 0} // it dies from loneliness.
	{state.asBoolean && (neighbours > 3)} {newState = 0} // it dies from overpopulation.
	{state.asBoolean.not && (neighbours == 3)} {newState = 1}; // birth
	// update
	cells[xIndex][yIndex] = newState;
};
 And now using the same SynthDef as before we can proceed with a drawFunc like this:
 .drawFunc_({ // setup UserView and callback func
	var speed = userView.frame % 4;
	synth.set(\array, cells.flatten); // sonify
	cells.do{arg xItem, xIndex;  // for each cell
		xItem.do{arg yItem, yIndex;
			if (yItem!=0) { // draw current state
				Pen.fillColor_(Color.new255(214,176,49));
	Pen.addRect(Rect(xIndex*20,yIndex*20,20,20);
				Pen.fill;
			}; 
			if (speed==0) {updateCell.(xIndex,yIndex);}; // calculate and draw new state
		};
	};
}); 
The full code for this example can be found online in this book packt code bundle. A still from the visualization follows:
 
9677OS_08_03.png
Summary
In this chapter, we dealt with more advanced topics such as how to perform statistical analyses on datasets, how to parse textual information, how to perform intelligent encodings and how to implement 1 and 2 dimensional cellular automata. The later are examples of generative systems that produce audio and video autonomously.
In the next chapter, we will implement a more sophisticated generative system as a case to demonstrate various design patterns and software architecture paradigms so that we are in position to properly design and implement code for more sophisticated systems.
